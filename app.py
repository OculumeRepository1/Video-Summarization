import os
import cv2
import base64
from PIL import Image
from flask import Flask, request, render_template
from werkzeug.utils import secure_filename
import ollama
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.cluster import DBSCAN
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_distances 
from flask import send_from_directory
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'mkv'}
sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda', model_kwargs={"torch_dtype": torch.float16})
dbscan_model = DBSCAN(eps=0.3, min_samples=1, metric='precomputed')
# --------- LLM 1: TinyVision Placeholder ---------
# def tinyVision(vmodel,frame,prompt,tokenizer):
#     enc_image = vmodel.encode_image(frame)
#     responcse=vmodel.answer_question(enc_image, prompt, tokenizer)
#     return responcse

import cv2
import base64

def ollama_summary_with_captions(captioned_frames, prompt):
    """
    Generates a high-level summary of a video using frame-by-frame image captions and their corresponding frames.

    Parameters:
    - captioned_frames (list): A list of tuples (caption: str, image: np.ndarray).
    - prompt (str): A user-defined instruction or question for the LLM to tailor the summary.

    Returns:
    - str: A textual summary generated by the LLM based on visual and captioned input.
    """
    if not captioned_frames:
        raise ValueError("No captioned frames provided.")

    print("Preparing prompt and images for LLM...")

    text_lines = []
    image_payload = []

    for i, (caption, image) in enumerate(captioned_frames):
        try:
            # Encode image to base64
            success, buffer = cv2.imencode('.jpg', image)
            if not success:
                print(f"Warning: Could not encode image at frame {i+1}. Skipping.")
                continue
            image_b64 = base64.b64encode(buffer).decode('utf-8')
        except Exception as e:
            print(f"Error encoding image at frame {i+1}: {e}")
            continue

        # Prepare payload for Ollama
        image_payload.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_b64}"
            }
        })

        # Add frame caption
        text_lines.append(f"Frame {i+1}: {caption}")

    full_caption_text = "\n".join(text_lines)

    final_prompt = f"""
    Based on the following frame-by-frame captions and images, generate a high-level summary of the video.
    Only use visual and caption information provided. Do not assume anything beyond these.

    User Instruction:
    {prompt}

    Captions:
    {full_caption_text}
    """.strip()

    response = ollama.chat(
        model='gemma3:4b',
        messages=[
            {
                "role": "user",
                "content": [{"type": "text", "text": final_prompt}] + image_payload
            }
        ]
    )

    return response['message']['content']


# --------- Helpers ---------
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS
def clustering_denoise(captions): # at the level of captions
    embeddings = sentence_model.encode(list(captions.values()), convert_to_tensor=True).cpu().numpy()

    distance_matrix = cosine_distances(embeddings)

    labels = dbscan_model.fit_predict(distance_matrix)

    indicator = merge_captions(labels.tolist())

    filtered_captions = {key: value for key, value, select in zip(captions.keys(), captions.values(), indicator) if select}

    return filtered_captions

def merge_captions(labels):
    n = len(labels)
    indicator = [True] * n

    for i in range(n):
        if labels[i] == -1:
            indicator[i] = False

    for i in range(n - 4):
        window = labels[i:i + 5]
        most_common_label = max(set(window), key=window.count)
        
        if window.count(most_common_label) == 4:
            for j in range(5):
                if window[j] != most_common_label:
                    indicator[i + j] = False
                    break

    return indicator
def ollama_model(prompt,image):
    #prompt3="Analyze the description {response}. If any of these conditions are identify. 1: Check for weapons, knife any other kind of weapons. 2: person is smoking, cigerete. 3: Person lying on the floor or person fallen. 4: Violence; if any are present, Just response with 'Yes'or 'No'."
    #image.show()
    #print(prompt3)
    res=ollama.chat(
    model='gemma3:4b',
    messages=[
        {
            'role':'user',
            'content':prompt,
           'images':[image],
            'max_tokens': 500
        }
    ]
    )
    response_json = res['message']['content']
    return response_json
def ollama_QA(text):
    #prompt3="Analyze the description {response}. If any of these conditions are identify. 1: Check for weapons, knife any other kind of weapons. 2: person is smoking, cigerete. 3: Person lying on the floor or person fallen. 4: Violence; if any are present, Just response with 'Yes'or 'No'."
    #image.show()
    #print(prompt3)
    # prompt = f"""
    #     You will be provided with a sequence of frame-by-frame image captions from a video.

    #     Your task is to generate a structured, high-level summary describing what is happening across the video.

    #     Please follow these instructions:
    #     - Use only the information explicitly mentioned in the captions.
    #     - Do not make any assumptions beyond the provided descriptions.
    #     - Focus on identifying key actions, transitions, or events over time.
    #     - Write in clear, concise sentences using a chronological narrative.
    #     - Group similar or repeating actions together where appropriate.

    #     Captions:
    #     \"\"\"{text}\"\"\"
    #     """
    prompt = f"""
        "Analyze the frame-by-frame descriptions of human activities below. Summarize the continuous activity into a single, cohesive sentence starting with the subject. Captions may contain errors, so ignore captions that are less frequently mentioned in the overall description. Avoid generating unreal or unsure interactions by disregarding any irrelevant information about human activity

        Captions:
        \"\"\"{text}\"\"\"
        """
    res=ollama.chat(
        model='gemma3:4b',
        messages=[
        {
            'role':'user',
            'content':prompt,
            'max_tokens': 800,
        }
        ]
    )
    return res['message']['content']
def extract_frames(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    count = 0
    interval = 120  # Extract every 30th frame
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if count % interval == 0:
            frames.append(frame)
        count += 1
    cap.release()
    return frames

model_id = "vikhyatk/moondream2"
revision = "2024-08-26"  # Pin to specific version
# # vmodel = AutoModelForCausalLM.from_pretrained(
# #     model_id, trust_remote_code=True, revision=revision
# # )
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas

def save_summary_to_pdf(summary_text, filename="summary.pdf"):
    pdf_path = os.path.join("uploads", filename)
    c = canvas.Canvas(pdf_path, pagesize=letter)
    width, height = letter
    lines = summary_text.split('\n')
    y = height - 50
    for line in lines:
        if y < 40:
            c.showPage()
            y = height - 50
        c.drawString(40, y, line[:100])  # avoid overflow
        y -= 15
    c.save()
    return filename


@app.route('/download/<filename>')
def download_pdf(filename):
    return send_from_directory(app.config['UPLOAD_FOLDER'], filename, as_attachment=True)
# # For gpus 
# vmodel = AutoModelForCausalLM.from_pretrained(
#     model_id, trust_remote_code=True, revision=revision,
#     torch_dtype=torch.float16
#     #attn_implementation="flash_attention_2"
# ).to("cuda")


# tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
# --------- Flask Routes ---------
@app.route("/", methods=["GET", "POST"])
def index():
    if request.method == "POST":
        file = request.files.get("video")
        prompt = request.form.get("prompt")
        prompt="Analyze the image carefully and describe the situation in detail and ignore non violent behavior. Indicate if there are any guns, weapons, knife, or any thing in mouth or smoking gesture or smoking substances or person lying on the floor or person  or fighting stance — but only mention them if you are completely certain, based on clear visual evidence. Do not speculate or fabricate. If a weapon is confidently detected, specify the type of weapon observed. Then, describe: What is happening in the image (the situation) and the actions being taken by individuals.Any visible information about the individuals (e.g., clothing, posture, interaction. Also include time stamp of the image in the description. If you are not sure about any of these, just don't mention it. If you are not sure about any of these, just don't mention it."
        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            filepath = os.path.join(app.config["UPLOAD_FOLDER"], filename)
            file.save(filepath)

            frames = extract_frames(filepath)
            print(f"Extracted {len(frames)} frames from the video.")
            if not frames:
                return render_template("index.html", error="No frames extracted from the video.")
            captioned_frames = {}
            for i, frame in enumerate(frames):
                try:
                    pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                    success, buffer = cv2.imencode('.jpg', frame)
                    img_base64 = base64.b64encode(buffer).decode('utf-8')
                    caption=ollama_model(prompt, img_base64)
                    print(f"Frame {i+1} caption: {caption}")
                    captioned_frames[i] = (caption)
                except Exception as e:
                    print(f"Error in frame {i}: {e}")
            outputs = clustering_denoise(captioned_frames)
            outputs = ", ".join(f"{key}: '{value}'" for key, value in outputs.items())
            prompt= f"Generate a high-level summary of the video based on the following frame-by-frame captions and images. Respond only based on visual information and captions provided.\n\nUser Prompt:\n{prompt}\n\nFrame Captions:\n"
            #summary=''.join([f"Frame {i+1}: {caption}" for i, (caption, _) in enumerate(captioned_frames)])
            summary = ollama_QA(outputs)
            pdf_filename = save_summary_to_pdf(summary)
            return render_template("index.html", summary=summary, pdf_file=pdf_filename)
        return render_template("index.html", error="Invalid file format")

    return render_template("index.html")
if __name__ == "__main__":
    app.run(debug=True)

# from flask import Flask, render_template, request, redirect, url_for
# from werkzeug.utils import secure_filename
# import os
# from summarization import summarize_file
# import cv2
# app = Flask(__name__)
# app.config['UPLOAD_FOLDER'] = 'uploads'
# app.config['ALLOWED_EXTENSIONS'] = {'mp4', 'avi', 'mov'}

# def allowed_file(filename):
#     return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

# @app.route("/", methods=["GET", "POST"])
# def index():
#     summary_result = None

#     if request.method == "POST":
#         if 'video' not in request.files:
#             return "❌ No file part"

#         file = request.files['video']

#         if file.filename == '':
#             return "❌ No selected file"

#         if file and allowed_file(file.filename):
#             filename = secure_filename(file.filename)
#             filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
#             file.save(filepath)

#             # Summarize the uploaded video
#             summary_result = summarize_file({
#                 "file_id": filepath,
#                 "chunk_duration": 20,
#                 "model": "llava:7b",
#                 "prompt": "Describe all human actions"
#             })

#     return render_template("index.html", summary=summary_result)

# if __name__ == "__main__":
#     os.makedirs("uploads", exist_ok=True)
#     app.run(debug=True)
