import os
import cv2
import base64
from PIL import Image
from flask import Flask, request, render_template
from werkzeug.utils import secure_filename
import ollama
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from sklearn.cluster import DBSCAN
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_distances 
from flask import send_from_directory
from video_utils import extract_frames, _compile_kb_regex, classify_incident
import json
import tempfile
import textwrap
project_dir = os.path.dirname(os.path.abspath(__file__))
app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

with open(os.path.join(project_dir, "Dict.json"), "r", encoding="utf-8") as f:
    KB = json.load(f)
    

INCIDENT_KB = _compile_kb_regex(KB)
ALLOWED_EXTENSIONS = {'mp4', 'avi', 'mov', 'mkv'}
sentence_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda', model_kwargs={"torch_dtype": torch.float16})
dbscan_model = DBSCAN(eps=0.3, min_samples=1, metric='precomputed')
# --------- LLM 1: TinyVision Placeholder ---------
# def tinyVision(vmodel,frame,prompt,tokenizer):
#     enc_image = vmodel.encode_image(frame)
#     responcse=vx = left_margin + indent_offset if i == 0 else left_marginmodel.answer_question(enc_image, prompt, tokenizer)
#     return responcse

import cv2
import base64

def ollama_summary_with_captions(captioned_frames, prompt):
    """
    Generates a high-level summary of a video using frame-by-frame image captions and their corresponding frames.

    Parameters:
    - captioned_frames (list): A list of tuples (caption: str, image: np.ndarray).
    - prompt (str): A user-defined instruction or question for the LLM to tailor the summary.

    Returns:
    - str: A textual summary generated by the LLM based on visual and captioned input.
    """
    if not captioned_frames:
        raise ValueError("No captioned frames provided.")

    print("Preparing prompt and images for LLM...")

    text_lines = []
    image_payload = []

    for i, (caption, image) in enumerate(captioned_frames):
        try:
            # Encode image to base64
            success, buffer = cv2.imencode('.jpg', image)
            if not success:
                print(f"Warning: Could not encode image at frame {i+1}. Skipping.")
                continue
            image_b64 = base64.b64encode(buffer).decode('utf-8')
        except Exception as e:
            print(f"Error encoding image at frame {i+1}: {e}")
            continue

        # Prepare payload for Ollama
        image_payload.append({
            "type": "image_url",
            "image_url": {
                "url": f"data:image/jpeg;base64,{image_b64}"
            }
        })

        # Add frame caption
        text_lines.append(f"Frame {i+1}: {caption}")

    full_caption_text = "\n".join(text_lines)

    final_prompt = f"""
    Based on the following frame-by-frame captions and images, generate a high-level summary of the video.
    Only use visual and caption information provided. Do not assume anything beyond these.

    User Instruction:
    {prompt}

    Captions:
    {full_caption_text}
    """.strip()

    response = ollama.chat(
        model='gemma3:4b',
        messages=[
            {
                "role": "user",
                "content": [{"type": "text", "text": final_prompt}] + image_payload
            }
        ]
    )

    return response['message']['content']


# --------- Helpers ---------
def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS
def clustering_denoise(captions): # at the level of captions
    embeddings = sentence_model.encode(list(captions.values()), convert_to_tensor=True).cpu().numpy()

    distance_matrix = cosine_distances(embeddings)

    labels = dbscan_model.fit_predict(distance_matrix)

    indicator = merge_captions(labels.tolist())

    filtered_captions = {key: value for key, value, select in zip(captions.keys(), captions.values(), indicator) if select}

    return filtered_captions

def merge_captions(labels):
    n = len(labels)
    indicator = [True] * n

    for i in range(n):
        if labels[i] == -1:
            indicator[i] = False

    for i in range(n - 4):
        window = labels[i:i + 5]
        most_common_label = max(set(window), key=window.count)
        
        if window.count(most_common_label) == 4:
            for j in range(5):
                if window[j] != most_common_label:
                    indicator[i + j] = False
                    break

    return indicator
def ollama_model(prompt,image):
    #prompt3="Analyze the description {response}. If any of these conditions are identify. 1: Check for weapons, knife any other kind of weapons. 2: person is smoking, cigerete. 3: Person lying on the floor or person fallen. 4: Violence; if any are present, Just response with 'Yes'or 'No'."
    #image.show()
    #print(prompt3)
    res=ollama.chat(
    model='gemma3:4b',
    messages=[
        {
            'role':'user',
            'content':prompt,
           'images':[image],
            'max_tokens': 500
        }
    ]
    )
    response_json = res['message']['content']
    return response_json
def ollama_QA(text,prompt):
    #prompt3="Analyze the description {response}. If any of these conditions are identify. 1: Check for weapons, knife any other kind of weapons. 2: person is smoking, cigerete. 3: Person lying on the floor or person fallen. 4: Violence; if any are present, Just response with 'Yes'or 'No'."
    #image.show()
    #print(prompt3)
    # prompt = f"""
    #     You will be provided with a sequence of frame-by-frame image captions from a video.

    #     Your task is to generate a structured, high-level summary describing what is happening across the video.

    #     Please follow these instructions:
    #     - Use only the information explicitly mentioned in the captions.
    #     - Do not make any assumptions beyond the provided descriptions.
    #     - Focus on identifying key actions, transitions, or events over time.
    #     - Write in clear, concise sentences using a chronological narrative.
    #     - Group similar or repeating actions together where appropriate.

    #     Captions:
    #     \"\"\"{text}\"\"\"
    #     """
    # prompt = f"""
    #     "Analyze the frame-by-frame descriptions of human activities below. Summarize the continuous activity into a single, cohesive sentence starting with the subject. Captions may contain errors, so ignore captions that are less frequently mentioned in the overall description. Avoid generating unreal or unsure interactions by disregarding any irrelevant information about human activity

    #     Captions:
    #     \"\"\"{text}\"\"\"
    #     """
    res=ollama.chat(
        model='gemma3:4b',
        messages=[
        {
            'role':'user',
            'content':prompt,
            'max_tokens': 2000,
        }
        ]
    )
    return res['message']['content']
def extract_frames(video_path):
    cap = cv2.VideoCapture(video_path)
    frames = []
    count = 0
    interval = 30  # Extract every 30th frame
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if count % interval == 0:
            frames.append(frame)
        count += 1
    cap.release()
    return frames

model_id = "vikhyatk/moondream2"
revision = "2024-08-26"  # Pin to specific version
# # vmodel = AutoModelForCausalLM.from_pretrained(
# #     model_id, trust_remote_code=True, revision=revision
# # )
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas


def save_summary_to_pdf(summary_text, filename="summary.pdf", images=None, image_captions=None):
    pdf_path = os.path.join("uploads", filename)
    c = canvas.Canvas(pdf_path, pagesize=letter)
    width, height = letter

    # Font settings
    c.setFont("Helvetica", 11)

    # Layout settings
    left_margin = 40
    right_margin = 40
    max_width = width - left_margin - right_margin
    line_height = 15
    y = height - 50

    # Indentation settings
    indent_offset = 20  # pixels
    avg_char_width = 6  # Approx for Helvetica 11pt
    max_chars_per_line = int(max_width / avg_char_width)

    # Split into paragraphs
    paragraphs = summary_text.strip().split('\n')

    # Process text paragraphs first
    for para in paragraphs:
        wrapped_lines = textwrap.wrap(para, width=max_chars_per_line)
        for i, line in enumerate(wrapped_lines):
            if y < 40:
                c.showPage()  # Go to the next page
                c.setFont("Helvetica", 11)
                y = height - 50

            # Apply indentation only to the first line of the paragraph
            x = left_margin + indent_offset if i == 0 else left_margin
            c.drawString(x, y, line)
            y -= line_height

    # After text, add images with captions at the end
    if images and image_captions:
        for i, opencv_image in enumerate(images):
            # Convert the OpenCV image to a temporary file (e.g., PNG)
            _, img_file = tempfile.mkstemp(suffix='.png')  # Create a temporary file
            cv2.imwrite(img_file, opencv_image)  # Save OpenCV image to temp file
            
            # Get image size for positioning
            img_width, img_height = 200, 200  # Default size, can adjust as needed
            y -= 20  # Space before the image

            # Check if there is space for the image
            if y - img_height < 40:  # If there isn't enough space, go to the next page
                c.showPage()
                c.setFont("Helvetica", 11)
                y = height - 50
            
            # Add the image to the PDF
            c.drawImage(img_file, left_margin, y - img_height, width=img_width, height=img_height)
            y -= img_height + 10  # Adjust y-coordinate after the image

            # Get the caption from the dictionary
            #caption = image_captions.get(i, "Image caption")  # Default caption if index not found
            # for key, value in image_captions.items():
            #     caption = value
            caption = image_captions[i] if i < len(image_captions) else "Image caption"
            # Wrap the caption text
            wrapped_caption = textwrap.wrap(caption, width=max_chars_per_line)
            c.setFont("Helvetica", 9)
            for line in wrapped_caption:
                if y < 40:  # If there's no space, go to the next page
                    c.showPage()
                    c.setFont("Helvetica", 9)
                    y = height - 50
                c.drawString(left_margin, y, line)
                y -= line_height  # Move down for the next line of caption

            y -= 15  # Space after the caption

            # Clean up the temporary image file
            os.remove(img_file)

    c.save()
    return filename


@app.route('/download/<filename>')
def download_pdf(filename):
    return send_from_directory(app.config['UPLOAD_FOLDER'], filename, as_attachment=True)
# # For gpus 
# vmodel = AutoModelForCausalLM.from_pretrained(
#     model_id, trust_remote_code=True, revision=revision,
#     torch_dtype=torch.float16
#     #attn_implementation="flash_attention_2"
# ).to("cuda")


# tokenizer = AutoTokenizer.from_pretrained(model_id, revision=revision)
# --------- Flask Routes ---------
@app.route("/", methods=["GET", "POST"])
def index():
    if request.method == "POST":
        file = request.files.get("video")
        user_prompt = request.form.get("prompt")
        if user_prompt is None or user_prompt.strip() == "":
            prompt=f"""Analyze the human behavior in the image. Is there any indication of violence, gun use, or drug use? 

            Instructions:
            - Begin your answer with "Yes" or "No".
            - If the answer is "Yes", describe the situation, detailing any signs of violence, weapons, or drug activity.
            - If the answer is "No", simply state, "No safety concern detected."
            - Always start your answer with "Yes" or "No"."""
        else:
            prompt=f"""Analyze the human behavior in the image. Is there any indication of violence, gun use, or drug use? 

                Instructions:
                - Begin your answer with "Yes" or "No".
                - If the answer is "Yes", describe the situation, detailing any signs of violence, weapons, or drug activity.
                - If the answer is "No", simply state, "No safety concern detected."
                - Always start your answer with "Yes" or "No".
                The user is looking for: {user_prompt}"""
        if file and allowed_file(file.filename):
            filename = secure_filename(file.filename)
            filepath = os.path.join(app.config["UPLOAD_FOLDER"], filename)
            file.save(filepath)

            frames = extract_frames(filepath)
            print(f"Extracted {len(frames)} frames from the video.")
            if not frames:
                return render_template("index.html", error="No frames extracted from the video.")
            captioned_frames = {}
            images_for_pdf = []
            captions=[]
            for i, frame in enumerate(frames):
                try:
                    pil_img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
                    success, buffer = cv2.imencode('.jpg', frame)
                    img_base64 = base64.b64encode(buffer).decode('utf-8')
                    caption=ollama_model(prompt, img_base64)
                    response2=caption[0:3]
                    # Normalize the answer
                    #resp_clean = response.strip().lower()
                    resp_clean = (caption or "").strip()
                                

                    # ----------- Filtering strategy -----------
                    labels, top_sev = classify_incident(resp_clean, INCIDENT_KB)
                    is_yes = resp_clean.startswith("yes")
                    is_critical = top_sev in ("critical", "high")  # decision comes from KB
                    #print(f"is_critical={is_critical}")
                    #print(f"[KB] labels={labels}, top_severity={top_sev}")
                    print(f"Orginal LLM Response {caption}")

                    # Use both the Gemma "Yes/No" AND the KB classification
                    first_char = response2[:1]  # safe

                    #print(f'First LLM Response {response1}')
                    print(f'Second LLM Response {response2}')
                    #if (top_sev in ("critical", "high")):
                    if response2[0]=="y" or response2[0]=="Y" or response2[0]=="H":
                        print(f"Frame {i+1} caption: {caption}")
                        captioned_frames[i] = (caption)
                        images_for_pdf.append(frame)
                        captions.append(caption)
                except Exception as e:
                    print(f"Error in frame {i}: {e}")
            outputs = clustering_denoise(captioned_frames)
            outputs = ", ".join(f"{key}: '{value}'" for key, value in outputs.items())
            prompt_s = f"""
                You are an assistant tasked with summarizing incidents from a video using frame-by-frame image captions and creating a comprehensive report.

                Instructions:
                - Review the frame captions provided below carefully.
                - Identify and highlight key incidents or significant events from the frames.
                - Write a clear, concise paragraph for each identified incident.
                - Start each paragraph with a **bolded incident title** (e.g., "Knife Found:") followed by a brief narrative. Include the approximate time, a description of the incident, and any relevant context from the frame captions.
                - After summarizing the incidents, compile the information into a **full, proper report**. Ensure the report is well-structured and cohesive, and provides a clear overview of the incidents.
                
                Example:
                **Knife Found:** At approximately 19:40, a youth arrived at the shelter for supper. During a routine search, staff discovered a knife tucked inside their backpack.

                Now, process the following:

                User Prompt: 
                {prompt}

                Frame Captions: 
                {captioned_frames}
            """
            #summary=''.join([f"Frame {i+1}: {caption}" for i, (caption, _) in enumerate(captioned_frames)])
            summary = ollama_QA(outputs,prompt_s)
            pdf_filename = save_summary_to_pdf(summary,images=images_for_pdf,image_captions=captions)
            return render_template("index.html", summary=summary, pdf_file=pdf_filename)
        return render_template("index.html", error="Invalid file format")

    return render_template("index.html")
if __name__ == "__main__":
    app.run(debug=True)

# from flask import Flask, render_template, request, redirect, url_for
# from werkzeug.utils import secure_filename
# import os
# from summarization import summarize_file
# import cv2
# app = Flask(__name__)
# app.config['UPLOAD_FOLDER'] = 'uploads'
# app.config['ALLOWED_EXTENSIONS'] = {'mp4', 'avi', 'mov'}

# def allowed_file(filename):
#     return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

# @app.route("/", methods=["GET", "POST"])
# def index():
#     summary_result = None

#     if request.method == "POST":
#         if 'video' not in request.files:
#             return "❌ No file part"

#         file = request.files['video']

#         if file.filename == '':
#             return "❌ No selected file"

#         if file and allowed_file(file.filename):
#             filename = secure_filename(file.filename)
#             filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
#             file.save(filepath)

#             # Summarize the uploaded video
#             summary_result = summarize_file({
#                 "file_id": filepath,
#                 "chunk_duration": 20,
#                 "model": "llava:7b",
#                 "prompt": "Describe all human actions"
#             })

#     return render_template("index.html", summary=summary_result)

# if __name__ == "__main__":
#     os.makedirs("uploads", exist_ok=True)
#     app.run(debug=True)
